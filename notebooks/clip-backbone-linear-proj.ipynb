{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#!pip install timm\n# !pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git\n!pip install albumentations","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:36:20.858394Z","iopub.execute_input":"2022-09-20T04:36:20.861117Z","iopub.status.idle":"2022-09-20T04:36:54.545908Z","shell.execute_reply.started":"2022-09-20T04:36:20.861023Z","shell.execute_reply":"2022-09-20T04:36:54.544168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"cleaning imagenet and landmark data according to https://arxiv.org/pdf/2003.11211.pdf\nhttps://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import transforms # datasets, models,\ncudnn.benchmark = True\nimport albumentations as A\n\nimport clip\nfrom clip.clip import _download, _MODELS\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nfrom pathlib import Path\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom timeit import default_timer as timer\n# import joblib\n\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:36:54.550004Z","iopub.execute_input":"2022-09-20T04:36:54.550443Z","iopub.status.idle":"2022-09-20T04:36:59.370743Z","shell.execute_reply.started":"2022-09-20T04:36:54.550404Z","shell.execute_reply":"2022-09-20T04:36:59.369285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip.available_models()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T03:59:11.723734Z","iopub.execute_input":"2022-09-20T03:59:11.724564Z","iopub.status.idle":"2022-09-20T03:59:11.736143Z","shell.execute_reply.started":"2022-09-20T03:59:11.724522Z","shell.execute_reply":"2022-09-20T03:59:11.735139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    RANDOM_SEED = 45\n\n  # backbone\n    IMAGE_SIZE = 224\n    BACKBONE_MODEL_NAME = 'ViT-L/14'\n    BACKBONE_MODEL = None\n  \n  # projection layer\n    N_CLASSES = 1000\n    EMB_DIM = 256\n\n  # training\n    TRAIN = True\n    RESUME = True\n    RESUME_STATES_DIR = './model_state.pkl'\n    BATCH_SIZE = 32\n    EPOCHS = 1\n    LR = .001\n    \n  # files\n    TRAIN_DIR = '../input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train'\n    VAL_DIR = '../input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val'\n    VAL_CSV = '../input/imagenet-object-localization-challenge/LOC_val_solution.csv'\n    OUTPUT_DIR = Path(\"./\")\n    LOGS_DIR = Path(OUTPUT_DIR, \"logs\")\n    MODEL_DIR = Path(OUTPUT_DIR, \"model\")\n    OOF_DIR = Path(OUTPUT_DIR, \"oof\")\n    LOGS_DIR.mkdir(parents=True, exist_ok=True)\n    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n    OOF_DIR.mkdir(parents=True, exist_ok=True)\n\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(config.RANDOM_SEED)\n\nprint(config.DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:40:08.497677Z","iopub.execute_input":"2022-09-20T04:40:08.498457Z","iopub.status.idle":"2022-09-20T04:40:08.519758Z","shell.execute_reply.started":"2022-09-20T04:40:08.498418Z","shell.execute_reply":"2022-09-20T04:40:08.518033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/openai/CLIP\n\nmodel_path = _download(_MODELS[config.BACKBONE_MODEL_NAME], os.path.expanduser(\"~/.cache/clip\"))\nwith open(model_path, 'rb') as opened_file:\n    clip_vit = torch.jit.load(opened_file, map_location=config.DEVICE).visual.eval()\n    \n    #model, preprocess = clip.load(config.BACKBONE_MODEL_NAME, device=config.DEVICE)\n    \n    config.BACKBONE_MODEL = clip_vit #model.to(config.DEVICE)\n    #config.BACKBONE_PREPROCESS = preprocess\n\n# https://github.com/rwightman/pytorch-image-models","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:40:09.517661Z","iopub.execute_input":"2022-09-20T04:40:09.518099Z","iopub.status.idle":"2022-09-20T04:40:38.196006Z","shell.execute_reply.started":"2022-09-20T04:40:09.518049Z","shell.execute_reply":"2022-09-20T04:40:38.194625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config.BACKBONE_MODEL","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:40:38.199384Z","iopub.execute_input":"2022-09-20T04:40:38.199906Z","iopub.status.idle":"2022-09-20T04:40:38.207308Z","shell.execute_reply.started":"2022-09-20T04:40:38.199860Z","shell.execute_reply":"2022-09-20T04:40:38.205609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config.BACKBONE_PREPROCESS","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:40:38.209996Z","iopub.execute_input":"2022-09-20T04:40:38.210523Z","iopub.status.idle":"2022-09-20T04:40:38.238622Z","shell.execute_reply.started":"2022-09-20T04:40:38.210407Z","shell.execute_reply":"2022-09-20T04:40:38.235642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://ryanwingate.com/intro-to-machine-learning/deep-learning-with-pytorch/loading-image-data-into-pytorch/\n# train_trans = transforms.Compose([transforms.RandomRotation(30),\n#                                        transforms.RandomResizedCrop(config.IMAGE_SIZE),\n#                                        transforms.RandomHorizontalFlip(),\n#                                        transforms.ToTensor(),\n#                                        transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n#                                                             [0.26862954, 0.26130258, 0.27577711])]\n#                                      )\n# val_trans = transforms.Compose([transforms.Resize(config.IMAGE_SIZE),\n#                                       #transforms.CenterCrop(config.IMAGE_SIZE),\n#                                       transforms.ToTensor(),\n#                                       transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n#                                                            [0.26862954, 0.26130258, 0.27577711])]\n#                                     )","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:40:38.243775Z","iopub.execute_input":"2022-09-20T04:40:38.245803Z","iopub.status.idle":"2022-09-20T04:40:38.260019Z","shell.execute_reply.started":"2022-09-20T04:40:38.245742Z","shell.execute_reply":"2022-09-20T04:40:38.258261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://albumentations.ai/docs/examples/migrating_from_torchvision_to_albumentations/\n#https://github.com/albumentations-team/albumentations/issues/687\n\n#https://aigeekprogrammer.com/pytorch-dataset-division-transformations-gpu-metric-visualization/\nclass ToTensorCustom(A.BasicTransform):\n    \"\"\"Convert image and mask to `torch.Tensor`\n    * Image numpy: [H, W, C] -> Image tensor: [C, H, W]\n\n    \"\"\"\n    def __init__(self, always_apply=True, p=1.0):\n        super().__init__(always_apply=always_apply, p=p)\n    \n    @property\n    def targets(self):\n        return {'image':self.apply}\n    \n    def apply(self, img, **params):\n        \"\"\"Image from numpy [H, W, C] to tensor [C, H, W]\"\"\"\n        return torch.from_numpy(img.transpose(2, 0, 1))\n    \ntrain_trans = A.Compose([A.Resize(400,400), \n                               A.RandomCrop(config.IMAGE_SIZE,config.IMAGE_SIZE),\n                               A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n                               A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n                               A.HorizontalFlip(p=0.5),\n                               A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n                              ToTensorCustom()])\nval_trans = A.Compose([A.Resize(config.IMAGE_SIZE,config.IMAGE_SIZE),\n                             #A.CenterCrop(config.IMAGE_SIZE,config.IMAGE_SIZE),\n#                               A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n#                               A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n#                               A.HorizontalFlip(p=0.5),\n                              A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n                              ToTensorCustom()])","metadata":{"execution":{"iopub.status.busy":"2022-09-20T04:40:38.262652Z","iopub.execute_input":"2022-09-20T04:40:38.264341Z","iopub.status.idle":"2022-09-20T04:40:38.305295Z","shell.execute_reply.started":"2022-09-20T04:40:38.263714Z","shell.execute_reply":"2022-09-20T04:40:38.292927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImagenetDataset(Dataset):\n    def __init__(self, data,transformation):\n        self.data = data\n        self.length = len(data)\n        self.trans = transformation\n        \n    def __getitem__(self,idx):\n        row = self.data.iloc[idx]\n        img = cv2.imread(row.path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.trans(image=img)\n        return img, row.label\n    \n    def __len__(self):\n        return self.length\n    \n    # will not work for test/val data which have no labels (?) and labels in csv respectively\ndef df_from_file_hierarchy(data_path,img_per_class_cap=50):\n    img_and_type = []\n    for img_class in tqdm(os.listdir(data_path)):\n        img_count = 0\n        for img_path in os.listdir(data_path+'/'+img_class):\n            full_path = data_path+'/'+img_class+'/'+img_path\n                \n            img_and_type.append({'path':full_path,'type':img_class})\n                \n            img_count+=1\n            if img_count == img_per_class_cap:\n                break\n        \n    df = pd.DataFrame(img_and_type)\n    return df\n    \ndef df_from_csv(csv_path,img_path_parent=None,img_type='.JPEG'):#,img_per_class_cap=50):\n    df = pd.read_csv(csv_path)\n    img_and_type = []\n    for i, row in df.iterrows():\n        img_and_type.append({'path':img_path_parent+'/'+row.ImageId+img_type,\n                                    'type':row.PredictionString.split()[0]})\n    df = pd.DataFrame(img_and_type)\n    return df\n\n# very slow loading\n# imagenet_train = ImageFolder(root = '../input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train')","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:37.607649Z","iopub.execute_input":"2022-09-20T05:02:37.608610Z","iopub.status.idle":"2022-09-20T05:02:37.632978Z","shell.execute_reply.started":"2022-09-20T05:02:37.608523Z","shell.execute_reply":"2022-09-20T05:02:37.631205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_datasets(out_of_train_class_count = 5):\n    df1 = df_from_file_hierarchy(config.TRAIN_DIR)\n    #df2 = df_from_csv(config.VAL_CSV,img_path_parent=config.VAL_DIR)\n    \n    #data_df = pd.concat([df1,df2])   --> I don't think I should concat. I should get a dictionary for labels?\n    \n    data_df = df1\n    data_df['label'] = LabelEncoder().fit_transform(data_df['type'])\n    \n    data_df.to_csv(\"data.csv\")\n    \n    out_of_train_labels = np.random.choice(data_df['label'].unique(),size=out_of_train_class_count, replace=False)\n    out_of_train_val_df = data_df.loc[data_df['label'].isin(out_of_train_labels)].reset_index()\n    in_train = data_df.loc[~data_df['label'].isin(out_of_train_labels)].reset_index()\n    \n    train_idxs = []\n    for label in tqdm(in_train['label'].unique()):\n        label_group = in_train.loc[in_train['label']==label]\n        selections = np.random.choice(len(label_group),int(len(label_group)*.9), replace=False)\n        train_idxs.extend([index for index in label_group.iloc[selections].index])\n    \n    train_df = in_train.iloc[train_idxs].reset_index()\n    val_df = in_train.loc[~in_train.index.isin(train_idxs)].reset_index()\n    \n    print('Train len:',len(train_df))\n    print('Val len:',len(val_df))\n    print('Oot val len:',len(out_of_train_val_df))\n    \n    return train_df,val_df,out_of_train_val_df","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:37.866717Z","iopub.execute_input":"2022-09-20T05:02:37.868893Z","iopub.status.idle":"2022-09-20T05:02:37.881515Z","shell.execute_reply.started":"2022-09-20T05:02:37.868842Z","shell.execute_reply":"2022-09-20T05:02:37.879718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df,val_df,oot_val_df = prepare_datasets()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:38.199012Z","iopub.execute_input":"2022-09-20T05:02:38.199625Z","iopub.status.idle":"2022-09-20T05:02:41.044479Z","shell.execute_reply.started":"2022-09-20T05:02:38.199577Z","shell.execute_reply":"2022-09-20T05:02:41.042897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:41.752116Z","iopub.execute_input":"2022-09-20T05:02:41.753071Z","iopub.status.idle":"2022-09-20T05:02:41.769620Z","shell.execute_reply.started":"2022-09-20T05:02:41.753019Z","shell.execute_reply":"2022-09-20T05:02:41.768003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:43.030005Z","iopub.execute_input":"2022-09-20T05:02:43.030437Z","iopub.status.idle":"2022-09-20T05:02:43.042776Z","shell.execute_reply.started":"2022-09-20T05:02:43.030403Z","shell.execute_reply":"2022-09-20T05:02:43.041410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImagenetDataset(train_df,train_trans)\nval_dataset = ImagenetDataset(val_df,val_trans)\noot_val_dataset = ImagenetDataset(oot_val_df,val_trans)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:43.951512Z","iopub.execute_input":"2022-09-20T05:02:43.952787Z","iopub.status.idle":"2022-09-20T05:02:43.960470Z","shell.execute_reply.started":"2022-09-20T05:02:43.952745Z","shell.execute_reply":"2022-09-20T05:02:43.959048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, \n                          batch_size=config.BATCH_SIZE,\n                          shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset,\n                       batch_size=config.BATCH_SIZE,\n                       shuffle=False, num_workers=2)\n# oot_val_loader = DataLoader(oot_val_dataset,\n#                        batch_size=config.BATCH_SIZE,\n#                        shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:44.732041Z","iopub.execute_input":"2022-09-20T05:02:44.733831Z","iopub.status.idle":"2022-09-20T05:02:44.741942Z","shell.execute_reply.started":"2022-09-20T05:02:44.733782Z","shell.execute_reply":"2022-09-20T05:02:44.740350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_loaders = {}\ndata_loaders['train']=train_loader\ndata_loaders['val']=val_loader","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:45.176268Z","iopub.execute_input":"2022-09-20T05:02:45.177347Z","iopub.status.idle":"2022-09-20T05:02:45.211533Z","shell.execute_reply.started":"2022-09-20T05:02:45.177312Z","shell.execute_reply":"2022-09-20T05:02:45.209898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_loader)*config.BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:02:45.676739Z","iopub.execute_input":"2022-09-20T05:02:45.677174Z","iopub.status.idle":"2022-09-20T05:02:45.687129Z","shell.execute_reply.started":"2022-09-20T05:02:45.677139Z","shell.execute_reply":"2022-09-20T05:02:45.685585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearProjection(nn.Module):\n    def __init__(self,image_size: int = 224):\n        super().__init__()\n        self.image_size=image_size\n        self.encoder = config.BACKBONE_MODEL\n        self.linear1 = nn.Linear(768,350)\n        self.linear2 = nn.Linear(350,1000)\n        self.pool = nn.AdaptiveAvgPool1d(64)\n        \n        # using cross entropy loss so not softmax\n        # self.softmax = nn.Softmax()\n\n    def forward(self, input, submission: bool=True):\n        if submission:\n            input = transforms.functional.resize(input,size=[self.image_size,self.image_size])\n            input = transforms.functional.normalize(input, mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n        \n        #with torch.no_grad():\n        output = self.encoder(input.half())#.encode_image(input)\n        output = F.relu(output)\n        output = self.linear1(output.float())\n        output = F.relu(output)\n        output = self.linear2(output.float())\n        \n        if submission:\n            output = self.pool(output)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:55:14.951709Z","iopub.execute_input":"2022-09-20T05:55:14.952179Z","iopub.status.idle":"2022-09-20T05:55:14.967218Z","shell.execute_reply.started":"2022-09-20T05:55:14.952145Z","shell.execute_reply":"2022-09-20T05:55:14.965156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import time\n# import copy\n# # https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n# def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n#     since = time.time()\n\n#     best_model_wts = copy.deepcopy(model.state_dict())\n#     best_acc = 0.0\n\n#     for epoch in range(num_epochs):\n#         print(f'Epoch {epoch}/{num_epochs - 1}')\n#         print('-' * 10)\n\n#         # Each epoch has a training and validation phase\n#         for phase in ['train', 'val']:\n#             if phase == 'train':\n#                 model.train()  # Set model to training mode\n\n#             else:\n#                 model.eval()   # Set model to evaluate mode\n                \n#             running_loss = 0.0\n#             running_corrects = 0\n            \n#             # Iterate over data.\n#             for inputs, labels in tqdm(dataloaders[phase]):\n#                 inputs = torch.transpose(inputs['image'],1,3)\n#                 #print(inputs.shape)\n#                 inputs = inputs.to(config.DEVICE)\n#                 labels = labels.to(config.DEVICE)\n                \n#                 # zero the parameter gradients\n#                 optimizer.zero_grad()\n\n#                 # forward\n#                 # track history if only in train\n#                 with torch.set_grad_enabled(phase == 'train'):\n#                     outputs = model(inputs,submission=False)\n#                     _, preds = torch.max(outputs, 1)\n                    \n#                     loss = criterion(outputs, labels)\n\n#                     # backward + optimize only if in training phase\n#                     if phase == 'train':\n#                         loss.backward()\n#                         optimizer.step()\n#                 # statistics\n#                 running_loss += loss.detach().item() * inputs.size(0)\n#                 running_corrects += torch.sum(preds == labels.data)\n                \n#                 gc.collect()\n                \n#             if phase == 'train':\n#                 scheduler.step()\n\n#             epoch_loss = running_loss / (len(dataloaders[phase])*config.BATCH_SIZE)\n#             epoch_acc = running_corrects.double() / (len(dataloaders[phase])*config.BATCH_SIZE)\n\n#             print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n#             # deep copy the model\n#             if phase == 'val' and epoch_acc > best_acc:\n#                 best_acc = epoch_acc\n#                 best_model_wts = copy.deepcopy(model.state_dict())\n                \n#     time_elapsed = time.time() - since\n#     print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n#     print(f'Best val Acc: {best_acc:4f}')\n\n#     # load best model weights\n#     model.load_state_dict(best_model_wts)\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:03:23.273688Z","iopub.execute_input":"2022-09-20T05:03:23.274337Z","iopub.status.idle":"2022-09-20T05:03:23.286109Z","shell.execute_reply.started":"2022-09-20T05:03:23.274286Z","shell.execute_reply":"2022-09-20T05:03:23.284546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearProjection().to(config.DEVICE)\n\ncriterion = nn.CrossEntropyLoss().to(config.DEVICE)\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:03:23.822591Z","iopub.execute_input":"2022-09-20T05:03:23.823015Z","iopub.status.idle":"2022-09-20T05:03:24.217200Z","shell.execute_reply.started":"2022-09-20T05:03:23.822981Z","shell.execute_reply":"2022-09-20T05:03:24.215529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(config.DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:03:24.952852Z","iopub.execute_input":"2022-09-20T05:03:24.953304Z","iopub.status.idle":"2022-09-20T05:03:24.961572Z","shell.execute_reply.started":"2022-09-20T05:03:24.953270Z","shell.execute_reply":"2022-09-20T05:03:24.959746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracies = []\nval_accuracies =[]\nstart = timer()\nfor epoch in range(config.EPOCHS):\n    model.train()\n    train_accuracy = []\n    for x, y in tqdm(data_loaders['train']):\n        x = x['image'].to(config.DEVICE)\n        y = y.to(config.DEVICE)\n        optimizer.zero_grad() \n        yhat = model.forward(x,submission=False)\n        loss = criterion(yhat, y)\n        loss.backward()\n        optimizer.step()\n        prediction = torch.argmax(yhat, dim=1)\n        train_accuracy.extend((y == prediction).detach().cpu().numpy())\n    train_accuracies.append(np.mean(train_accuracy)*100)\n\n    # for every epoch we do a validation step to asses accuracy and overfitting\n    model.eval()\n    with torch.no_grad():\n        val_accuracy = []  # accuracies for each batch of validation dataset\n        for vx, vy in tqdm(data_loaders['val']):\n            vx = vx['image'].to(config.DEVICE)\n            vy = vy.to(config.DEVICE)\n            yhat = model.forward(vx,submission=False)\n            prediction = torch.argmax(yhat, dim=1)\n            # to numpy in order to use next the vectorized np.mean\n            val_accuracy.extend((vy == prediction).detach().cpu().numpy())\n        val_accuracies.append(np.mean(val_accuracy)*100)\n   # simple logging during training\n    print(f'Epoch #{epoch+1}. Train accuracy: {np.mean(train_accuracy)*100:.2f}. \\\n                      Validation accuracy: {np.mean(val_accuracy)*100:.2f}')\nend = timer()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:03:25.404578Z","iopub.execute_input":"2022-09-20T05:03:25.405900Z","iopub.status.idle":"2022-09-20T05:25:26.922421Z","shell.execute_reply.started":"2022-09-20T05:03:25.405850Z","shell.execute_reply":"2022-09-20T05:25:26.918709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.save(model.state_dict(),'model_state.pkl')\nstates = model.state_dict().copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:39:09.999643Z","iopub.execute_input":"2022-09-20T05:39:10.001207Z","iopub.status.idle":"2022-09-20T05:39:10.014054Z","shell.execute_reply.started":"2022-09-20T05:39:10.001154Z","shell.execute_reply":"2022-09-20T05:39:10.012529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_to_sub = LinearProjection()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:55:22.220019Z","iopub.execute_input":"2022-09-20T05:55:22.220503Z","iopub.status.idle":"2022-09-20T05:55:22.237513Z","shell.execute_reply.started":"2022-09-20T05:55:22.220434Z","shell.execute_reply":"2022-09-20T05:55:22.236145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_to_sub.load_state_dict(states)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:55:22.915534Z","iopub.execute_input":"2022-09-20T05:55:22.916016Z","iopub.status.idle":"2022-09-20T05:55:22.953853Z","shell.execute_reply.started":"2022-09-20T05:55:22.915982Z","shell.execute_reply":"2022-09-20T05:55:22.951657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_model = torch.jit.script(model_to_sub)\nsaved_model.save('saved_model.pt')\n\n# saved_model = torch.jit.load('saved_model.pt').cuda()\n# input = torch.randint(0, 255, test_input_size, device='cuda', dtype=torch.uint8)\n\n# with torch.no_grad():\n#     output = saved_model(input)\n#     print(output.dtype, output.shape, output.mean().item())\n#     assert output.shape == torch.Size([test_input_size[0], 64])\n#     torch.cuda.synchronize()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:56:09.869823Z","iopub.execute_input":"2022-09-20T05:56:09.870293Z","iopub.status.idle":"2022-09-20T05:56:12.741577Z","shell.execute_reply.started":"2022-09-20T05:56:09.870259Z","shell.execute_reply":"2022-09-20T05:56:12.739520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from zipfile import ZipFile\nwith ZipFile('submission.zip','w') as zip_file:\n    zip_file.write('./saved_model.pt', arcname='saved_model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-09-20T05:56:18.975701Z","iopub.execute_input":"2022-09-20T05:56:18.976144Z","iopub.status.idle":"2022-09-20T05:56:26.010743Z","shell.execute_reply.started":"2022-09-20T05:56:18.976095Z","shell.execute_reply":"2022-09-20T05:56:26.008651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}